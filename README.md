[image1]: assets/trained_agent.gif
[image2]: assets/2.png

# Deep Reinforcement Learning Project - Continuous Control

## Content
- [Introduction](#intro)
- [Unity Environment](#unitity_env)
- [Files in the Repo](#files_in_repo)
- [Watch trained Agents](#trained_agents)
- [Setup Instructions](#Setup_Instructions)
- [Acknowledgments](#Acknowledgments)
- [Further Links](#Further_Links)

## Introduction <a name="what_is_reinforcement"></a>
- Reinforcement learning is **learning** what to do — **how to map situations to actions** — so as **to maximize a numerical reward** signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. (Sutton and Barto, [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html))
- Deep reinforcement learning refers to approaches where the knowledge is represented with a deep neural network

- This project is part of the Udacity Nanodegree program 'Deep Reinforcement Learning'. Please check this [link](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893?utm_source=gsem_brand&utm_medium=ads_r&utm_campaign=12906460312_c&utm_term=121838875579&utm_keyword=deep%20reinforcement%20udacity_e&gclid=CjwKCAjw-e2EBhAhEiwAJI5jg7Ycb934lFlosCFVpvwKRD_U5ESjMX18faGkkTTUkIyZVJ6yU4HkohoCyfIQAvD_BwE) for more information.

## Unity Environment <a name="unitity_env"></a>
- [Unity Machine Learning Agents (ML-Agents)](https://github.com/Unity-Technologies/ml-agents) is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents.
- Implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games
- For this project, an **agent should be trained to keep track on a moving target**.
- There are two versions of the environment:
    - **Option 1**: In order to solve the environment, **one single agent** must get an average score of +30 over 100 consecutive episodes.
    - **Option 2**: In order to solve the environment, **multiple agents** must get an average score of +30 over 100 consecutive episodes and overr all agents.
- Here: **Option 2 was chosen** (20 agent version)
- A **reward** of **+0.1** is provided for for each step that the agent's hand is in the goal location.
- **Goal**: Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.
- The **state space** has **33 dimensions** and contains the **agent's position**, **rotation**, **velocity**, **angular-velocity** of the arm. Given this information, the agent has to learn how to best select actions.
- Each **action** is a vector with four numbers
- Every **entry in the action vector** must be a number between **-1 and 1**.
    ```
    INFO:unityagents:
    'Academy' started successfully!
    Unity Academy name: Academy
            Number of Brains: 1
            Number of External Brains : 1
            Lesson number : 0
            Reset Parameters :
            goal_speed -> 1.0
            goal_size -> 5.0
    Unity brain name: ReacherBrain
            Number of Visual Observations (per agent): 0
            Vector Observation space type: continuous
            Vector Observation space size (per agent): 33
            Number of stacked Vector Observation: 1
            Vector Action space type: continuous
            Vector Action space size (per agent): 4
            Vector Action descriptions: , , ,
    ```
- The task is **episodic**, and in order to solve the environment, the agent **must get an average score of +30 over 100 consecutive episodes**.

## Files in the repo <a name="files_in_repo"></a>
The workspace contains the following files:
- **README.md**: Markdown file, the readme of this repo.
- **Report.md**: Markdown file, a detailed description of the code implementation.
- **requiremens.txt**: txt file containing python packages needed for this repo.  
- **/assets/...**: Folder containing images and gifs needed for README.md and Report.md.
- **/notebooks_python/Continuous_Control.ipynb**: Main notebook file to train 20 agents based on a DDPG approach.
- **/notebooks_python/Continuous_Control_Trained_Agent.ipynb**: Second notebook file to watch the behaviour of trained agents.
- **/notebooks_python/ddpg_agent.py**: Python file containing the implementation of the deep reinforcement learning agent based on DDPG.
- **/notebooks_python/model.py**: Python file containing the PyTorch Deep Learning models generated by deep neural networks that acts as a function approximators for actor and critic
    - two actor neural networks (local and target) which map states to action values for policy training and
    - two critic neural networks (local and target) which map states to Q-values for action-value training
- **/notebooks_python/checkpoint_actor.pth**: PyTorch state_dict file containing trained weights for the actor networks.
- **/notebooks_python/checkpoint_critic.pth**: PyTorch state_dict file containing trained weights for the critic networks.
- **/notebooks_python/Reacher_Windows_x86_64/**: Downloaded Unity Environment needed to watch a trained agent.
- **/notebooks_python/unity-environment.log**: Log file repoting the last interaction with the environment.


## Watch trained Agents <a name="trained_agents"></a>
- The video below demonstrates the result of trained agents. All 20 agents learned to follow its corresponding green ball.

    ![image1]

- Below one can see the scoring plot.

    ![image2]

    - **Score** has been averaged over all 20 agents.
    - **Moving Average** is the rolling average with a roll length of 10, i.e. the **Moving Average** is calculated from 10 consecutive **Score** values.   
- After **21 episodes** the moving average was always **greater than 30**. Therefore the task (average score over all 20 agents AND higher than 30 for at least 100 episodes) has been **successfully completed at episode 121**.

## Setup Instructions <a name="Setup_Instructions"></a>
The following is a brief set of instructions on setting up a cloned repository.

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites: Installation of Python via Anaconda and Command Line Interaface <a name="Prerequisites"></a>
- Install [Anaconda](https://www.anaconda.com/distribution/). Install Python - 64 Bit

    ```
    $ conda upgrade conda
    $ conda upgrade --all
    ```

- Optional: In case of trouble add Anaconda to your system path. Write in your Command Line Interface (CLI)
    ```
    $ export PATH="/path/to/anaconda/bin:$PATH"
    ```

### Clone the project <a name="Clone_the_project"></a>
- Open your CLI
- Change Directory to your project older, e.g. `cd my_github_projects`
- Clone the Github Project inside this folder via:
    ```
    $ git clone https://github.com/ddhartma/Deep-Reinforcement-Learning-Project-Continuous-Control.git
    ```

- Change Directory
    ```
    $ cd Deep-Reinforcement-Learning-Project-Continuous-Control
    ```

### Create (and activate) a new environment:
- Create a Python 3.6 environment via conda
    ```
    $ conda create --name drlnd python=3.6
    $ conda activate drlnd
    ```

- To install ```requirements.txt``` in the new environment, use **pip** installed within the environment. Install pip first by
    ```
    $ conda install pip
    ```

- Install all packages provided in ```requirements.txt``` (via pip) needed to train and watch a smart agent
    ```
    $ pip install -r requirements.txt
    ```

- Install torch via conda. Please use the [conda command](https://pytorch.org/) adapted to your platform
    ```
    $ conda install pytorch=0.4.0 -c pytorch
      or for example
    $ conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
    ```

- Clone the repository (if you haven't already!), and navigate to the python/ folder. Then, install several dependencies.
    ```
    git clone https://github.com/udacity/deep-reinforcement-learning.git
    cd deep-reinforcement-learning/python
    pip install .
    ```

- Create an IPython kernel for the **drlnd** environment.
    ```
    $ python -m ipykernel install --user --name drlnd --display-name "drlnd"
    ```

- Check the environment installation via
    ```
    $ conda list
    ```
### Download the Unity environment
- Only if do not use Windows: The Unity environment in this repo is prepared for Windows. In case you do not use Windows download the Unity Environment. You need only select the environment that matches your operating system (Version 1: One Agent)
    - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux.zip)
    - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher.app.zip)
    - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)
    - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86_64.zip)
- Then, place the file in the ...notebooks/ folder of this repository, and unzip (or decompress) the file.

- Unzip this file and place it in the ```/notebooks_python```` folder of this repo
- Replace **<file_name>** in the following line of ```Continuous_Control.ipynb```.
    ```
    env = UnityEnvironment(file_name="<file_name>"
    ```
    with the filepath of the executable Unity evironment file(**path/to/Reacher.exe** or **path/to/Reacher.app** for example).
    Save the notebooks.

### To Start Agent Training
- Navigate via CLI to ```Continuous_Control.ipynb```
- Type in terminal
    ```
    $ jupyter notebook Continuous_Control.ipynb
    ```
- Run each cell in the notebook to train the agent.

### To Watch a Smart Agent
- Navigate via CLI to ```Continuous_Control_TRAINED.ipynb```
- Type in terminal
    ```
    $ jupyter notebook Continuous_Control_TRAINED.ipynb
    ```
- Run each cell in the notebook to watch a trained and smart agent.

## Acknowledgments <a name="Acknowledgments"></a>
* This project is part of the Udacity Nanodegree program 'Deep Reinforcement Learning'. Please check this [link](https://www.udacity.com) for more information.

## Further Links <a name="Further_Links"></a>
Git/Github
* [GitFlow](https://datasift.github.io/gitflow/IntroducingGitFlow.html)
* [A successful Git branching model](https://nvie.com/posts/a-successful-git-branching-model/)
* [5 types of Git workflows](https://buddy.works/blog/5-types-of-git-workflows)

Docstrings, DRY, PEP8
* [Python Docstrings](https://www.geeksforgeeks.org/python-docstrings/)
* [DRY](https://www.youtube.com/watch?v=IGH4-ZhfVDk)
* [PEP 8 -- Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/)

Further Deep Reinforcement Learning References
* [Very good summary of DQN](https://medium.com/@nisheed/udacity-deep-reinforcement-learning-project-1-navigation-d16b43793af5)
* [An Introduction to Deep Reinforcement Learning](https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c)
* Helpful medium blog post on policies [Off-policy vs On-Policy vs Offline Reinforcement Learning Demystified!](https://kowshikchilamkurthy.medium.com/off-policy-vs-on-policy-vs-offline-reinforcement-learning-demystified-f7f87e275b48)
* [Understanding Baseline Techniques for REINFORCE](https://medium.com/@fork.tree.ai/understanding-baseline-techniques-for-reinforce-53a1e2279b57)
* [Cheatsheet](https://raw.githubusercontent.com/udacity/deep-reinforcement-learning/master/cheatsheet/cheatsheet.pdf)
* [Reinforcement Learning Cheat Sheet](https://towardsdatascience.com/reinforcement-learning-cheat-sheet-2f9453df7651)
* [Reinforcement Learning Textbook](https://s3-us-west-1.amazonaws.com/udacity-drlnd/bookdraft2018.pdf)
* [Reinforcement Learning Textbook - GitHub Repo to Python Examples](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)
* [Udacity DRL Github Repository](https://github.com/udacity/deep-reinforcement-learning)
* [Open AI Gym - Installation Guide](https://github.com/openai/gym#installation)
* [Deep Reinforcement Learning Nanodegree Links](https://docs.google.com/spreadsheets/d/19jUvEO82qt3itGP3mXRmaoMbVOyE6bLOp5_QwqITzaM/edit#gid=0)

Important publications
* [2004 Y. Ng et al., Autonomoushelicopterflightviareinforcementlearning --> Inverse Reinforcement Learning](https://people.eecs.berkeley.edu/~jordan/papers/ng-etal03.pdf)
* [2004 Kohl et al., Policy Gradient Reinforcement Learning for FastQuadrupedal Locomotion --> Policy Gradient Methods](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/icra04.pdf)
* [2013-2015, Mnih et al. Human-level control through deep reinforcementlearning --> DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
* [2014, Silver et al., Deterministic Policy Gradient Algorithms --> DPG](http://proceedings.mlr.press/v32/silver14.html)
* [2015, Lillicrap et al., Continuous control with deep reinforcement learning --> DDPG](https://arxiv.org/abs/1509.02971)
* [2015, Schulman et al, High-Dimensional Continuous Control Using Generalized Advantage Estimation --> GAE](https://arxiv.org/abs/1506.02438)
* [2016, Schulman et al., Benchmarking Deep Reinforcement Learning for Continuous Control --> TRPO and GAE](https://arxiv.org/abs/1604.06778)
* [2017, PPO](https://openai.com/blog/openai-baselines-ppo/)
* [2018, Bart-Maron et al., Distributed Distributional Deterministic Policy Gradients](https://openreview.net/forum?id=SyZipzbCb)
* [2013, Sergey et al., Guided Policy Search --> GPS](https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf)
* [2015, van Hasselt et al., Deep Reinforcement Learning with Double Q-learning --> DDQN](https://arxiv.org/abs/1509.06461)
* [1993, Truhn et al., Issues in Using Function Approximation for Reinforcement Learning](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf)
* [2015, Schaul et al., Prioritized Experience Replay --> PER](https://arxiv.org/abs/1511.05952)
* [2015, Wang et al., Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)
* [2016, Silver et al., Mastering the game of Go with deep neural networks and tree search](https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search)
* [2017, Hessel et al. Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)
* [2016, Mnih et al., Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)
* [2017, Bellemare et al., A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)
* [2017, Fortunato et al., Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295)
* [2016 Wang et al., Sample Efficient Actor-Critic with Experience Replay --> ACER](https://arxiv.org/abs/1611.01224)
